---
title: 'COVID Scraping'
author: 'Jeffrey Brennan'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "C:/Users/jeffb/Desktop/Life/personal-projects/COVID")

library(data.table)
library(readxl)
library(dplyr)
```

# COVIDTRACKING 

```{r}
# TOOD: investigate 
# https://covidtracking.com/race
```


# CENSUS 

## County level

```{r}
# sourced from https://www.census.gov/data/tables/time-series/demo/popest/2010s-counties-detail.html
county_demo = read.csv('original-sources/census/county_demo.csv')

# restrict to 2018 pop estimate & drop extra cols
county_demo_2018 = subset(county_demo[, c(5:ncol(county_demo))], YEAR == 11)

# drop year
county_demo_2018$YEAR = NULL

# drop 'county' suffix
county_demo_2018$CTYNAME = gsub(' County', '', county_demo_2018$CTYNAME)

# drop agegrp = 0 (total row)
county_demo_2018 = county_demo_2018[which(county_demo_2018$AGEGRP != 0), ]

# rename cols for merging
colnames(county_demo_2018)[1] = 'County'

# add age labels
county_demo_2018$AGEGRP = factor(county_demo_2018$AGEGRP,
                                 labels = c('0-4', '5-9', '10-14', '15-19', '20-24', '25-29',
                                            '30-34', '35-39', '40-44', '45-49', '50-54', '55-59',
                                            '60-64',  '65-69', '70-74', '75-79', '80-84', '85+'))
```


## City level

```{r}
city_pops = read.csv('original-sources/census/city_pops.csv')

# keep only 2018 estimate
city_pops2 = city_pops[, c(3, 4, 9, 21)]
colnames(city_pops2) = c('County', 'Place_Code', 'City', 'Population')

# drop city, town, suffixes
city_pops2$City = gsub(' city| town|', '', city_pops2$City)

# drop pt suffix
# pt indicates overlap of cities between counties
city_pops2$City = gsub(' (pt.)', '', city_pops2$City, fixed = TRUE)

colnames(city_pops2)[1] = 'County_Code'

# get county fips
county_fips = subset(city_pops2, Place_Code == 0)[-1, c(1,3)]
colnames(county_fips) = c('County_Code', 'County')


# replace county fips with actual names
city_pops3 = merge(city_pops2, county_fips, by = 'County_Code')

# drop county totals from list
city_pops4 = subset(city_pops3, Place_Code != 0 & Place_Code != 99990)

# drop county suffix
city_pops4$County = gsub(' County', '', city_pops4$County)
```


# COUNTY LEVEL

## Google mobility

Used fread for faster import - very large file

```{r}
mobility_data = fread('https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv')
```

```{r}
# filter mobility
mobility_texas = subset(mobility_data, sub_region_1 == 'Texas')

# drop cols
mobility_texas = mobility_texas[, -c(1,2,5,6)]
write.csv(mobility_texas, 'original-sources/GOOGLE.csv', row.names = F)

# fix colnames
# values represent percent change from baseline
colnames(mobility_texas) = c('State', 'County', 'Date', 'Retail_Recreation', 'Grocery_Pharmacy', 'Parks', 'Transit', 'Workplaces', 'Residential')

# Add name for blank county cells & drop 'county' suffix
mobility_texas$County = sub('^$', 'Unallocated', mobility_texas$County)
mobility_texas$County = gsub(' County', '', mobility_texas$County)

#fix types
mobility_texas$State = as.factor(mobility_texas$State)
mobility_texas$County = as.factor(mobility_texas$County)
mobility_texas$Date = as.Date(mobility_texas$Date)
```


## DSHS 

```{r}
# cases
case_url = 'http://dshs.texas.gov/coronavirus/TexasCOVID19DailyCountyCaseCountData.xlsx'
temp = tempfile()
download.file(case_url, temp, mode = 'wb') 
DSHS_cases_time = data.frame(readxl::read_excel(temp, sheet = 1))

# fix colnames
colnames(DSHS_cases_time) = DSHS_cases_time[2, ]
DSHS_cases_time = DSHS_cases_time[3:(nrow(DSHS_cases_time) - 11), ]
colnames(DSHS_cases_time)[1] = 'County'

#save
write.csv(DSHS_cases_time, 'original-sources/DSHS_county_cases.csv', row.names = FALSE)

# melt
DSHS_cases_long = reshape::melt(DSHS_cases_time, id = c('County', 'Population'))
                                
# set colnames
colnames(DSHS_cases_long) = c('County', 'Population', 'Date', 'Cases_Cumulative')

# fix dates
DSHS_cases_long$Date = as.Date(gsub('Cases|\r|\r|\n', '', DSHS_cases_long$Date), format = '%m-%d')

# sort
DSHS_cases_long = DSHS_cases_long[order(DSHS_cases_long$County, DSHS_cases_long$Date), ]

# calculate daily cases
DSHS_cases_long$Cases_Cumulative = as.integer(as.character(DSHS_cases_long$Cases_Cumulative))
DSHS_cases_long = 
    DSHS_cases_long %>% 
    group_by(County) %>% 
     mutate(Cases_Daily = c(Cases_Cumulative[1], diff(Cases_Cumulative)))
```

```{r}
# deaths
death_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID19DailyCountyFatalityCountData.xlsx'
temp = tempfile()
download.file(death_url, temp, mode = 'wb') 
DSHS_deaths_time = data.frame(readxl::read_excel(temp, sheet = 1))

# fix colnames
colnames(DSHS_deaths_time) = DSHS_deaths_time[2, ]
DSHS_deaths_time = DSHS_deaths_time[3:(nrow(DSHS_deaths_time) - 10), ]
colnames(DSHS_deaths_time)[1] = 'County'

#save
write.csv(DSHS_deaths_time, 'original-sources/DSHS_county_deaths.csv', row.names = FALSE)

# melt
DSHS_deaths_long = reshape::melt(DSHS_deaths_time, id = c('County', 'Population'))
                                
# set colnames
colnames(DSHS_deaths_long) = c('County', 'Population', 'Date', 'Deaths_Cumulative')

# fix dates
DSHS_deaths_long$Date = as.Date(as.integer(DSHS_deaths_long$Date), origin = '2020-03-03')


# calculate daily deaths
DSHS_deaths_long$Deaths_Cumulative = as.integer(as.character(DSHS_deaths_long$Deaths_Cumulative))
DSHS_deaths_long = 
    DSHS_deaths_long %>% 
    group_by(County) %>%
    mutate(Deaths_Daily = c(Deaths_Cumulative[1], diff(Deaths_Cumulative)))
```

```{r}
# testing
test_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID-19CumulativeTestsOverTimebyCounty.xlsx'
temp = tempfile()
download.file(test_url, temp, mode = 'wb') 
DSHS_tests_time = data.frame(readxl::read_excel(temp, sheet = 1))

# TEMP FIX drop first duplicated date column
dupe_dates = which(DSHS_tests_time[1, ] == 'Tests Through June 23')
DSHS_tests_time = DSHS_tests_time[, -dupe_dates[1]]

# fix colnames
colnames(DSHS_tests_time) = DSHS_tests_time[1, ]
DSHS_tests_time = DSHS_tests_time[2:(nrow(DSHS_tests_time) - 9), ]
colnames(DSHS_tests_time)[1] = 'County'

#save
write.csv(DSHS_tests_time, 'original-sources/DSHS_county_tests.csv', row.names = FALSE)

# melt
DSHS_tests_long = reshape::melt(DSHS_tests_time, id = c('County'))
                                
# set colnames
colnames(DSHS_tests_long) = c('County', 'Date', 'Tests_Cumulative')

# fix dates
DSHS_tests_long$Date = gsub('*', '', DSHS_tests_long$Date, fixed = T)
DSHS_tests_long$Date = as.Date(gsub('Tests Through ', '', DSHS_tests_long$Date), format = '%B%d')

# replace '--' and '-' with NA
DSHS_tests_long$Tests_Cumulative = na_if(DSHS_tests_long$Tests_Cumulative, '-')
DSHS_tests_long$Tests_Cumulative = na_if(DSHS_tests_long$Tests_Cumulative, '--')

DSHS_tests_long$Tests_Cumulative = as.numeric(as.character(DSHS_tests_long$Tests_Cumulative))

# calculate daily tests
DSHS_tests_long = 
    DSHS_tests_long %>% 
    group_by(County) %>% 
    mutate(Tests_Daily = c(Tests_Cumulative[1], diff(Tests_Cumulative)))
```


```{r}
# active cases
active_case_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID-19ActiveCaseDatabyCounty.xlsx'
temp = tempfile()
download.file(active_case_url, temp, mode = 'wb') 
DSHS_active_cases_time = data.frame(readxl::read_excel(temp, sheet = 1))

# fix colname
colnames(DSHS_active_cases_time) = DSHS_active_cases_time[2, ]
DSHS_active_cases_time = DSHS_active_cases_time[3:(nrow(DSHS_active_cases_time)), 2:ncol(DSHS_active_cases_time)]
colnames(DSHS_active_cases_time)[1] = 'County'

#save
write.csv(DSHS_active_cases_time, 'original-sources/DSHS_county_active_cases.csv', row.names = FALSE)

# melt
DSHS_active_cases_long = reshape::melt(DSHS_active_cases_time, id = c('County'))
                                
# set colnames
colnames(DSHS_active_cases_long) = c('County', 'Date', 'Active_Cases_Cumulative')

# fix dates
DSHS_active_cases_long$Date = as.Date(gsub('Active|Cases|\r|\n', '', DSHS_active_cases_long$Date),
                                      format = '%m-%d')


# calculate daily active_cases
DSHS_active_cases_long$Active_Cases_Cumulative = as.integer(as.character(DSHS_active_cases_long$Active_Cases_Cumulative))
DSHS_active_cases_long =  
    DSHS_active_cases_long %>% 
    group_by(County) %>% 
    mutate(Active_Cases_Daily = c(Active_Cases_Cumulative[1], diff(Active_Cases_Cumulative)))
```

```{r}
# combine DSHS sources
DSHS_county_counts = Reduce(function(x, y) merge(x, y, by = c('County', 'Date'), all=TRUE),
       list(DSHS_cases_long, DSHS_deaths_long, DSHS_tests_long, DSHS_active_cases_long))


# drop population - will be using census data
DSHS_county_counts$Population.x = NULL
DSHS_county_counts$Population.y = NULL
```

### merging

```{r}
merged_counts = as.data.frame(merge(DSHS_county_counts, mobility_texas[, c(2:9)], by = c('County', 'Date'), all = TRUE))

# fix types
merged_counts$County = as.factor(merged_counts$County)
```

### Classifications

```{r}
# 6/16: Drop uneeded 
# add urban/rural, metro, border designations from https://www.dshs.state.tx.us/chs/info/TxCoPhrMsa.xls]\
# county_classifications = read_xlsx('original-sources/metro_classifications.xlsx', sheet = 1)[, c(1, 4:5, 8, 10:11)]
county_classifications = read_xlsx('original-sources/metro_classifications.xlsx', sheet = 1)[, c(1, 8)]


# set colnames
# colnames(county_classifications) = c('County', 'Public_Health_Region', 'Health_Service_Region', 'Metro_Area', 'NCHS_Classification', 'Border')
colnames(county_classifications) = c('County', 'Metro_Area')
```


```{r}
merged_counts2 = merge(merged_counts, county_classifications, by = 'County')
```

```{r}
# TSA levels
tsa = read.csv('original-sources/tsa_list.csv', header = F)[-1]
tsa_long = reshape::melt(tsa, id = c('V2', 'V3'))
tsa_long_complete = subset(tsa_long, value != '')[, c(1, 2, 4)]
colnames(tsa_long_complete) = c('TSA', 'TSA_Name', 'County')

tsa_long_complete$County = trimws(tsa_long_complete$County)
tsa_long_complete$TSA_Name = trimws(tsa_long_complete$TSA_Name)


merged_counts3 = merge(merged_counts2, tsa_long_complete, by = 'County', all = TRUE)

# sort by county, date
merged_counts3 = merged_counts3[order(merged_counts3$County, merged_counts3$Date), ]
```

```{r}
# DSHS pop
dshs_pops = unique(DSHS_cases_long[, c(1:2)])
colnames(dshs_pops) = c('County', 'Population_DSHS')

# # census pop
# census_pops = county_demo_2018[, c(1, 3)] %>% group_by(County) %>% dplyr::summarize(sum(TOT_POP))
# colnames(census_pops) = c('County', 'Population_Census')
# census_pops$County = as.factor(census_pops$County)

# add populations
merged_counts4 = merge(merged_counts3, dshs_pops, by = 'County')
# merged_counts4 = merge(merged_counts4, as.data.frame(census_pops), by = 'County')

merged_counts4$Population_DSHS = as.numeric(merged_counts4$Population_DSHS)
```


```{r}
# add TSA combination
merged_counts4$TSA_Combined = paste0(merged_counts4$TSA, ' - ', merged_counts4$TSA_Name)
```


# TSA LEVEL

## Computed

```{r}
DSHS_tsa_counts =
    merged_counts4 %>%
    group_by(TSA, TSA_Name, Date) %>%
    summarize_at(vars(Cases_Cumulative, Cases_Daily,
                      Deaths_Cumulative, Deaths_Daily,
                      Tests_Cumulative, Tests_Daily,
                      Active_Cases_Cumulative, Active_Cases_Daily),
                 funs(sum))

DSHS_tsa_pops = 
  subset(merged_counts4, Date == '2020-03-04') %>%
  group_by(TSA) %>%
  summarize_at(vars(Population_DSHS),
               funs(sum))

DSHS_tsa_google = 
  merged_counts4 %>%
  group_by(TSA, TSA_Name, Date) %>%
  summarize_at(vars(Retail_Recreation, Grocery_Pharmacy,
                    Parks, Transit,
                    Workplaces, Residential),
               funs(weighted.mean(., Population_DSHS)), na.rm = TRUE)

DSHS_tsa = merge(DSHS_tsa_counts, DSHS_tsa_google, by = c('TSA', 'TSA_Name', 'Date'))
DSHS_tsa = merge(DSHS_tsa, DSHS_tsa_pops, by = 'TSA', all = TRUE)
```


## DSHS hospitals

```{r}
hosp_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID-19HospitalizationsOverTimebyTSA.xlsx'
temp = tempfile()
download.file(hosp_url, temp, mode = 'wb') 
DSHS_tsa_hosp = data.frame(readxl::read_excel(temp, sheet = 1))

# fix colnames
colnames(DSHS_tsa_hosp) = DSHS_tsa_hosp[2, ]
DSHS_tsa_hosp = DSHS_tsa_hosp[3:(nrow(DSHS_tsa_hosp) - 1), ]
colnames(DSHS_tsa_hosp)[1] = 'TSA'

# drop . from TSA & drop TSA name
DSHS_tsa_hosp$TSA = gsub('.', '', DSHS_tsa_hosp$TSA, fixed = TRUE)
DSHS_tsa_hosp$`TSA Name` = NULL


#save
write.csv(DSHS_tsa_hosp, 'original-sources/DSHS_tsa_hosp.csv', row.names = FALSE)

# melt
DSHS_hosp_long = reshape::melt(DSHS_tsa_hosp, id = c('TSA'))
                                
# set colnames
colnames(DSHS_hosp_long) = c('TSA', 'Date', 'Hospitalizations_Daily')

# fix dates
DSHS_hosp_long$Date = as.Date(as.integer(DSHS_hosp_long$Date), origin = '2020-04-07')
```


### merging

```{r}
DSHS_hosp_long$TSA = as.factor(DSHS_hosp_long$TSA)
DSHS_tsa_merged = merge(DSHS_tsa, DSHS_hosp_long, by = c('Date', 'TSA'), all = TRUE)

DSHS_tsa_merged2 = DSHS_tsa_merged %>% distinct()
DSHS_tsa_merged3 = subset(DSHS_tsa_merged2, Date > as.Date('2020-03-03'))
```


# METRO LEVEL

## computed

```{r}
DSHS_metro_counts =
    merged_counts4 %>%
    group_by(Metro_Area, Date) %>%
    summarize_at(vars(Cases_Cumulative, Cases_Daily,
                      Deaths_Cumulative, Deaths_Daily,
                      Tests_Cumulative, Tests_Daily,
                      Active_Cases_Cumulative, Active_Cases_Daily),
                 funs(sum))

DSHS_metro_pops = 
  subset(merged_counts4, Date == '2020-03-04') %>%
  group_by(Metro_Area) %>%
  summarize_at(vars(Population_DSHS),
               funs(sum))

DSHS_metro_google = 
  merged_counts4 %>%
  group_by(Metro_Area, Date) %>%
  summarize_at(vars(Retail_Recreation, Grocery_Pharmacy,
                    Parks, Transit,
                    Workplaces, Residential),
               funs(weighted.mean(., Population_DSHS)), na.rm = TRUE)

DSHS_metro = merge(DSHS_metro_counts, DSHS_metro_google, by = c('Metro_Area', 'Date'))
DSHS_metro = subset(merge(DSHS_metro, DSHS_metro_pops, by = 'Metro_Area', all = TRUE), !is.na(Date))
```


# STATE LEVEL

## DSHS (time series)

```{r}
download.file('https://www.dshs.state.tx.us/coronavirus/TexasCOVID19CaseCountData.xlsx',
               destfile = 'original-sources/DSHS_state.xlsx', mode = 'wb')

library(readxl)
read_excel_allsheets = function(filename, tibble = FALSE) {
    sheets = readxl::excel_sheets(filename)
    x = lapply(sheets, function(X) readxl::read_excel(filename, sheet = X, skip = 1, col_names = TRUE, na = '.'))
    x = lapply(x, as.data.frame)
    return(x)
}

DSHS_state = read_excel_allsheets('original-sources/DSHS_state.xlsx')
```


```{r}
DSHS_tests = DSHS_state[[4]]
DSHS_tests = DSHS_tests[1:(nrow(DSHS_tests)-3), ]

colnames(DSHS_tests) = c('Date', 'Viral_Tests', 'Antibody_Tests', 'Tests_Total', 'Positive_Rate', 'Viral_Tests_New_Avg', 'Antibody_Tests_New_Avg', 'Total_Tests_New_Avg')

# fix date (https://stackoverflow.com/questions/43230470/how-to-convert-excel-date-format-to-proper-date-in-r) 
DSHS_tests$Date = as.Date(as.integer(DSHS_tests$Date), origin = '1899-12-30')

# fix calc col type
DSHS_tests$Positive_Rate = as.numeric(DSHS_tests$Positive_Rate)
```


```{r}
# drop row num
DSHS_hospitalizations = DSHS_state[[8]][, c(2:3)]

# drop footnote
DSHS_hospitalizations = DSHS_hospitalizations[1:(nrow(DSHS_hospitalizations)-2), ]

# fix date
DSHS_hospitalizations$Date = as.Date(DSHS_hospitalizations$Date, format = '%Y%m%d')

# set colnames
colnames(DSHS_hospitalizations)[2] = 'Hospital_Total'
```


```{r}
# merge tests & hospitalizations
DSHS_state_time = merge(DSHS_tests, DSHS_hospitalizations, by = 'Date', all = TRUE)
```

## DSHS (day counts)

```{r}
# avoids duplication of results between sheets
DSHS_state_day = data.frame(Recovered_Total = DSHS_state[[3]][1,1],
                            Active_Total = DSHS_state[[3]][1,2],
                            Tests_State_Lab = DSHS_state[[5]][1,2],
                            Tests_Commercial_Lab = DSHS_state[[5]][2,2],
                            Antibody_Tests_Positive = DSHS_state[[6]][2,2],
                            Hospital_Bed_Total = DSHS_state[[7]][2,2],
                            Hospital_Bed_Available = DSHS_state[[7]][3,2],
                            ICU_Bed_Available = DSHS_state[[7]][4,2],
                            Ventilator_Available = DSHS_state[[7]][5,2],
                            Case_Investigations = DSHS_state[[9]][[14,2]],
                            Death_Investigations = DSHS_state[[12]][14,2])
```

## DSHS Demographics

### Cases

```{r}
#Age
DSHS_case_age = DSHS_state[[9]][1:(nrow(DSHS_state[[9]])-4), ]
colnames(DSHS_case_age)[1] = 'Group'
DSHS_case_age$Count_Type = 'Case'

#Gender
DSHS_case_gender = DSHS_state[[10]][1:(nrow(DSHS_state[[10]])-4), ]
colnames(DSHS_case_gender)[1] = 'Group'
DSHS_case_gender$Count_Type = 'Case'

# Race
DSHS_case_race = DSHS_state[[11]][1:(nrow(DSHS_state[[11]])-4), ]
colnames(DSHS_case_race)[1] = 'Group'
DSHS_case_race$Count_Type = 'Case'
```

### Deaths

```{r}
#Age
DSHS_death_age = DSHS_state[[12]][1:(nrow(DSHS_state[[12]])-4), ]
colnames(DSHS_death_age)[1] = 'Group'
DSHS_death_age$Count_Type = 'Death'

#Gender
DSHS_death_gender = DSHS_state[[13]][1:(nrow(DSHS_state[[13]])-4), ]
colnames(DSHS_death_gender)[1] = 'Group'
DSHS_death_gender$Count_Type = 'Death'

# Race
DSHS_death_race = DSHS_state[[14]][1:(nrow(DSHS_state[[14]])-4), ]
colnames(DSHS_death_race)[1] = 'Group'
DSHS_death_race$Count_Type = 'Death'
```

```{r}
# combine 
DSHS_age = rbind(DSHS_case_age, DSHS_death_age)
DSHS_gender = rbind(DSHS_case_gender, DSHS_death_gender)
DSHS_race = rbind(DSHS_case_race, DSHS_death_race)
```

## Computed

```{r}
state_counts =
    merged_counts4 %>%
    group_by(Date) %>%
    summarize_at(vars(Cases_Cumulative, Cases_Daily,
                      Deaths_Cumulative, Deaths_Daily,
                      Tests_Cumulative, Tests_Daily,
                      Active_Cases_Cumulative, Active_Cases_Daily),
                 funs(sum))

state_pops = 
  subset(merged_counts4, Date == '2020-03-04') %>%
  group_by(Date) %>%
  summarize_at(vars(Population_DSHS),
               funs(sum))

state_google = 
  merged_counts4 %>%
  group_by(Date) %>%
  summarize_at(vars(Retail_Recreation, Grocery_Pharmacy,
                    Parks, Transit,
                    Workplaces, Residential),
               funs(weighted.mean(., Population_DSHS)), na.rm = TRUE)

merged_state = merge(state_counts, state_google, by = 'Date', all = TRUE)
merged_state2 = merge(merged_state, DSHS_state_time, by = 'Date', all = TRUE)

merged_state2$Population_DSHS = state_pops$Population_DSHS
```


# OUTPUT 

## City

```{r}
write.csv(city_pops4, 'combined-datasets/city_pops.csv', row.names = F)
```

## County

```{r}
write.csv(merged_counts4, file = 'combined-datasets/county.csv', row.names = F)
write.csv(county_demo_2018, file = 'combined-datasets/county_demo.csv', row.names = F)
```

## TSA

```{r}
write.csv(DSHS_tsa_merged3, file = 'combined-datasets/tsa.csv', row.names = F)
```

## Metro

```{r}
write.csv(DSHS_metro, file = 'combined-datasets/metro.csv', row.names = F)
```

## State

```{r}
library(xlsx)
write.xlsx(merged_state2, file="combined-datasets/state.xlsx", sheetName="longitudinal", row.names=FALSE)
write.xlsx(DSHS_state_day, file="combined-datasets/state.xlsx", sheetName="current", append = TRUE, row.names=FALSE)
```

### Demographics

```{r}
write.xlsx(DSHS_age, file="combined-datasets/demographics.xlsx", sheetName="age", row.names=FALSE)
write.xlsx(DSHS_gender, file="combined-datasets/demographics.xlsx", sheetName="gender", append=TRUE, row.names=FALSE)
write.xlsx(DSHS_race, file="combined-datasets/demographics.xlsx", sheetName="race", append=TRUE, row.names=FALSE)
```


## TABLEAU ONLY

```{r}
tableau_county = merged_counts4
write.csv(tableau_county, 'tableau/county.csv', row.names = FALSE)

# tableau_state = merged_state2[, c(1, 16:23)]
# write.csv(tableau_state, 'tableau/state_simple.csv', row.names = FALSE)
```


## Long format TSA & Metro

```{r}
# read in data
county = read.csv('combined-datasets/county.csv')
metro = read.csv('combined-datasets/metro.csv')
tsa = read.csv('combined-datasets/tsa.csv')

# combine into df
metro_long = merge(county, metro, by = c('Metro_Area', 'Date'), all = TRUE)
tsa_long = merge(county, tsa, by = c('TSA', 'TSA_Name', 'Date'), all = TRUE)

# drop county cols and suffixes
metro_long = metro_long[, -(grep('.x', colnames(metro_long), fixed = TRUE))]
colnames(metro_long) = gsub('.y', '', colnames(metro_long), fixed = TRUE)

# drop county cols and suffixes
tsa_long = tsa_long[, -(grep('.x', colnames(tsa_long), fixed = TRUE))]
colnames(tsa_long) = gsub('.y', '', colnames(tsa_long), fixed = TRUE)

tsa_long$TSA_Combined = paste0(tsa_long$TSA, ' - ', tsa_long$TSA_Name)

# output
# write.csv(metro_long, 'tableau/metro_long.csv', row.names = FALSE)
# write.csv(tsa_long, 'tableau/tsa_long.csv', row.names = FALSE)
```


```{r}
# transform names
colnames(metro)[3:length(colnames(metro))] = paste0('METRO_', colnames(metro)[3:length(colnames(metro))])
colnames(tsa)[4:length(colnames(tsa))] = paste0('TSA_', colnames(tsa)[4:length(colnames(tsa))])

# combine into df
combined_df = merge(county, metro, by = c('Metro_Area', 'Date'), all = TRUE)

tsa$TSA_Combined = paste0(tsa$TSA, ' - ', tsa$TSA_Name)

combined_df2 = merge(combined_df, tsa, by = c('TSA', 'TSA_Name', 'Date'), all = TRUE)

# output
# write.csv(combined_df2, 'tableau/combined.csv', row.names = FALSE)
```
