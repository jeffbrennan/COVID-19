---
title: 'COVID Scraping'
author: 'Jeffrey Brennan'
output: html_document
editor_options: 
  chunk_output_type: console
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "diagnostics/",
                    knit_root_dir = "C:/Users/jeffb/Desktop/Life/personal-projects/COVID") })
---

# SETUP

```{r, echo = FALSE}
# performance analysis 
# source: https://bookdown.org/yihui/rmarkdown-cookbook/time-chunk.html
all_times <- list()  # store the time for each chunk
knitr::knit_hooks$set(time_it = local({
  now <- NULL
  function(before, options) {
    if (before) {
      now <<- Sys.time()
    } else {
      res <- difftime(Sys.time(), now)
      all_times[[options$label]] <<- res
    }
  }
}))
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(time_it = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(comment = NA)
```

```{r, echo = FALSE}
refactor_version = '2'
```

```{r}
# data manipulation
library(data.table)
library(readxl)
library(writexl)
library(dplyr)
library(stringr)
library(zoo)
library(tidyr)

# plotting
library(ggplot2)
library(ggpubr)

# web scraping
library(rvest)
library(jsonlite)
```

```{r}
# Grab every sheet from an excel file and convert to list of dataframes
# https://stackoverflow.com/questions/12945687/read-all-worksheets-in-an-excel-workbook-into-an-r-list-with-data-frames
read_excel_allsheets = function(filename, tibble = FALSE) {
    sheets = readxl::excel_sheets(filename)
    x = lapply(sheets, function(X) read_excel(filename, sheet = X, skip = 1,
                                              col_names = TRUE, na = '.'))
    x = lapply(x, as.data.frame)
    return(x)
}

# set date for writing files
# If before 5 PM, then record as last date since DSHS data will not be updated yet
# TODO: see if this can be removed in place of using the max date on a consistent dataframe
date_out = ifelse((Sys.time() < as.POSIXct(paste0(Sys.Date(), '16:00'), tz = 'America/Chicago')),
                   Sys.Date() - 1,
                   Sys.Date())

# convert numeric sys.date() to yyyy-mm-dd
date_out = as.Date(date_out, origin = '1970-1-1')
```

## Classifications

```{r}
# add metro and PHR code designations
# source: https://www.dshs.state.tx.us/chs/info/TxCoPhrMsa.xls
# add PHR readable names from https://dshs.texas.gov/regions/default.shtm
PHR_helper = data.frame(PHR = c("1", "2/3", "4/5N",
                                "6/5S", "7", "8",
                                "9/10", "11"),
                        PHR_Name = c('Lubbock PHR', 'Arlington PHR', 'Tyler PHR',
                                     'Houston PHR', 'Temple PHR', 'San Antonio PHR',
                                     'El Paso PHR', 'Harlingen PHR'))


county_classifications = read_xlsx('original-sources/helpers/county_classifications.xlsx', sheet = 1) %>% 
  slice(1:254) %>% 
  select(1, 5, 8) %>%
  setNames(c('County', 'PHR', 'Metro_Area')) %>% 
  left_join(., PHR_helper, by = 'PHR') %>%
  mutate(PHR_Combined = paste0(PHR, ' - ', PHR_Name))
```

```{r}
# TSA levels
tsa_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID-19HospitalizationsOverTimebyTSA.xlsx'
temp = tempfile()
download.file(tsa_url, temp, mode = 'wb')
DSHS_tsa_names = readxl::read_xlsx(temp)[3:24, 1:2]

colnames(DSHS_tsa_names) = c('TSA', 'TSA_Name')

# drop . suffix from TSA codes
DSHS_tsa_names$TSA = gsub('.', '', DSHS_tsa_names$TSA, fixed = TRUE)

# get list of counties per TSA
tsa = read.csv('original-sources/helpers/tsa_list.csv', header = F)[-1]
tsa_long = reshape::melt(tsa, id = c('V2', 'V3'))
tsa_long_complete = subset(tsa_long, value != '')[, c(1, 4)] %>% 
  setNames(c('TSA', 'County')) %>%
  mutate(County = trimws(County)) %>% 
  left_join(DSHS_tsa_names, by = 'TSA') %>% 
  distinct() %>% 
  mutate(TSA_Combined = paste0(TSA, ' - ', TSA_Name))
```

# HOSPITAL LEVEL

```{r}
# TODO: finalize metrics of interest and add to tableau folder

hosp_dates = seq(as.Date('2020-12-07'), length = 52, by = 'week')
hosp_date = max(hosp_dates[which(hosp_dates <= date_out)])
hosp_date_f = format(hosp_date, '%Y%m%d')
#

# https://healthdata.gov/sites/default/files/reported_hospital_capacity_admissions_facility_level_weekly_average_timeseries_20201214.csv
base_hosp_url = 'https://healthdata.gov/sites/default/files/reported_hospital_capacity_admissions_facility_level_weekly_average_timeseries_'
hosp_df = fread(paste0(base_hosp_url, hosp_date_f, '.csv'), na.strings = c('-999999', '-999999.0'))

fips_link = read.csv('original-sources/helpers/unpublished/county_fips.csv') %>%
  dplyr::select(fips, county_name) %>%
  setNames(c('fips_code', 'County'))

hosp_df_parsed = hosp_df %>%
  filter(state == 'TX') %>%
  dplyr::select(-ccn, -state, -hospital_subtype, -is_metro_micro,
                -total_patients_hospitalized_confirmed_influenza_and_covid_7_day_avg,
                -contains('_sum'), -contains('_coverage')) %>%
  mutate(inpatient_beds_used_pct = inpatient_beds_used_7_day_avg / all_adult_hospital_inpatient_beds_7_day_avg) %>% 
  mutate(total_patients_hospitalized_confirmed_and_suspected_covid_7_day_avg
           = total_adult_patients_hospitalized_confirmed_and_suspected_covid_7_day_avg
           + total_pediatric_patients_hospitalized_confirmed_and_suspected_covid_7_day_avg) %>% 
  mutate(total_patients_hospitalized_confirmed_covid_7_day_avg
           = total_adult_patients_hospitalized_confirmed_covid_7_day_avg
           + total_pediatric_patients_hospitalized_confirmed_covid_7_day_avg) %>% 
  mutate(total_pediatric_icu_beds_7_day_avg
           = icu_beds_used_7_day_avg
           - total_staffed_adult_icu_beds_7_day_avg) %>% 
  relocate(inpatient_beds_used_pct, .after = inpatient_beds_used_7_day_avg) %>%
  relocate(total_patients_hospitalized_confirmed_and_suspected_covid_7_day_avg,
           .after = total_pediatric_patients_hospitalized_confirmed_and_suspected_covid_7_day_avg) %>% 
  relocate(total_patients_hospitalized_confirmed_covid_7_day_avg, 
           .after = total_pediatric_patients_hospitalized_confirmed_covid_7_day_avg) %>% 
  relocate(total_pediatric_icu_beds_7_day_avg, .after = icu_beds_used_7_day_avg) %>%
  setNames(gsub('_7_day_avg', '', names(.))) %>%
  left_join(fips_link, by = 'fips_code') %>%
  mutate(County = gsub(' County', '', County)) %>%
  left_join(tsa_long_complete %>% dplyr::select(County, TSA_Combined), by = 'County') %>%
  left_join(county_classifications %>% dplyr::select(County, PHR_Combined), by = 'County') %>%
  relocate(County, .after = collection_week) %>%
  relocate(TSA_Combined, .after = County) %>%
  relocate(PHR_Combined, .after = TSA_Combined) %>%
  relocate(fips_code, .after = PHR_Combined)
```

# hosp investigation

```{r}
# harris_hosp = hosp_df_parsed %>% filter(County == 'Harris')


tmc_hospital_list = c('HARRIS HEALTH SYSTEM', 'HOUSTON METHODIST HOSPITAL',
                  'UNIVERSITY OF TEXAS M D ANDERSON CANCER CENTER,THE', 'MEMORIAL HERMANN TEXAS MEDICAL CENTER',
                  "ST LUKE'S PATIENTS MEDICAL CENTER", 'TEXAS CHILDRENS HOSP')

# childrens = hosp_df_parsed %>% 
#   filter(hospital_name == 'TEXAS CHILDRENS HOSP') %>% 
#   dplyr::select(c(collection_week, total_staffed_adult_icu_beds_7_day_avg, total_icu_beds_7_day_avg))
# 
# coverage = hosp_df_parsed %>% 
#   dplyr::select(hospital_name, collection_week, contains('_coverage'))

hhs_tmc = hosp_df_parsed %>% 
  filter(hospital_name %in% tmc_hospital_list)


write.csv(hosp_df_parsed,
          'C:/Users/jeffb/Desktop/Life/personal-projects/COVID/original-sources/helpers/unpublished/hhs_hospitals.csv',
          row.names = FALSE)


write.csv(hhs_tmc,
          'C:/Users/jeffb/Desktop/Life/personal-projects/COVID/original-sources/helpers/unpublished/hhs_tmc_comparison.csv',
          row.names = FALSE)

# missing_tmc = c('UTHEALTH')


# ggplot(comparison_df, aes(x = ))

# memorial = harris_hosp %>% 
#   filter(hospital_name == 'UNITED MEMORIAL MEDICAL CENTER')%>%
#   filter(collection_week == as.Date('2020-12-11'))

# TODO: compare hospital list against TMC data

# harris_hosp_table = tableby(hospital_subtype ~ .,  data = harris_hosp)
# write2html(harris_hosp_table,
#            file = "C:/Users/jeffb/Desktop/Life/personal-projects/COVID/original-sources/helpers/unpublished/harris_hospitals.html",
#            title = 'City Hospital Comparison in Harris County')
```



# SCHOOL LEVEL

```{r}
# school files are small (~ 6kb - read every time and don't overwrite)
nyt_schools = rbindlist(lapply(list.files('original-sources/historical/nyt/archive', full.names = TRUE), read.csv)) %>% 
  setNames(c('School', 'City', 'County', 'Deaths_Cumulative', 'Cases_Cumulative', 'Date')) %>%
  mutate(Date = as.Date(Date)) %>% 
  mutate(Cases_Cumulative = ifelse(Cases_Cumulative == -1, NA, Cases_Cumulative))
```

## Texas school districts

```{r}
# data ending on Sunday posted on Friday (5 day lag)
school_dates = seq(as.Date('2020-10-11'), by = 'week', length.out = 52)
# no 11/22 update due to Thanksgiving
# school_dates = school_dates[-10]
school_date = max(school_dates[which(school_dates <= date_out - 5)])

tryCatch({
  school_url = paste0('https://dshs.texas.gov/chs/data/tea/district-level-school-covid-19-case-data/campus-level-Data-',
                    format(school_date+2, '%m%d%Y'), '.xls')
  # school_url = 'https://dshs.texas.gov/chs/data/tea/district-level-school-covid-19-case-data/District-level-data.xlsx'
  temp = tempfile()
  download.file(school_url, temp, mode = 'wb')
  DSHS_schools_raw = data.frame(read_excel(temp, sheet = 1))
  },
  error = function(e) {
   school_url = paste0('https://dshs.texas.gov/chs/data/tea/district-level-school-covid-19-case-data/campus-level-Data_',
                    format(school_date+2, '%m%d%Y'), '.xls')
  temp = tempfile()
  download.file(school_url, temp, mode = 'wb')
  DSHS_schools_raw <<- data.frame(read_excel(temp, sheet = 1))
  })


first_row = which(DSHS_schools_raw[, 1] == 'District Name') + 1
last_row = which(DSHS_schools_raw[, 1] == 'ZEPHYR ISD')
colnames(DSHS_schools_raw)[1:2] = c('District', 'LEA')


# read in matching county data (overlapping counties available for public, but not private ISDs)
# public school source: http://tea-texas.maps.arcgis.com/sharing/rest/content/items/87c957f2ec334c83bc2b952bf6e64344/data
# private school source: http://mansfield.tea.state.tx.us/Tea.AskTed.Web/Forms/DownloadDefault.aspx
county_isd_wide = read.csv('original-sources/helpers/county_isd_wide.csv')

DSHS_schools_clean = DSHS_schools_raw[first_row:last_row, ] %>%
  mutate(LEA = as.numeric(gsub("'", '', LEA))) %>%
  mutate(District = str_to_title(District)) %>%
  mutate(District = gsub('Isd', 'ISD', District)) %>%
  mutate(District = gsub('\n', ' ', District)) %>%
  mutate(Date = school_date) %>%
  # dplyr::select(District, Date, County, LEA, everything()) %>%
  dplyr::select(District, Date, LEA, everything()) %>%
  mutate_at(5:ncol(.), as.numeric) %>%
  setNames(c('District', 'Date', 'LEA', 'Total_Enrollment', 'Approximate_Enrollment',
             'Cases_Weekly_GRADE_EE_3', 'Cases_Weekly_GRADE_4_6', 'Cases_Weekly_GRADE_7_12',
             'Cases_Weekly_Staff', 'Infections_Weekly_On_Campus',
             'Infections_Weekly_Off_Campus', 'Infections_Weekly_Unknown',

             'Cases_Cumulative_GRADE_EE_3', 'Cases_Cumulative_GRADE_4_6',
             'Cases_Cumulative_GRADE_7_12', 'Cases_Cumulative_Staff',
             'Infections_Cumulative_On_Campus', 'Infections_Cumulative_Off_Campus',
             'Infections_Cumulative_Unknown'))

# save for future combinations - avoids cleaning on every run
write.csv(DSHS_schools_clean,
          paste0('original-sources/historical/dshs-schools/DSHS_Schools_', school_date, '.csv'),
          row.names = FALSE)
```


```{r}
# combine previous cleaned versions w/ current week
DSHS_school_df = rbindlist(lapply(list.files('original-sources/historical/dshs-schools', full.names = TRUE), read.csv))

DSHS_school_df = DSHS_school_df %>% 
  mutate(District = gsub('\n', ' ', District, fixed = TRUE)) %>% 
  mutate(District = gsub('  ', ' ', District)) %>% 
  mutate(District = gsub('Lighthouse Charter School', 'Lighthouse Public Schools', District)) %>% 
  mutate(Total_Enrollment = as.numeric(Total_Enrollment)) %>% 
  mutate(Approximate_Enrollment = as.numeric(Approximate_Enrollment)) %>% 
  mutate(Total_Enrollment = ifelse(Total_Enrollment == 0, NA, Total_Enrollment)) %>% 
  mutate(Approximate_Enrollment = ifelse(Approximate_Enrollment == 0, NA, Approximate_Enrollment)) %>%
  distinct()

write_xlsx(list('school_data' = DSHS_school_df,
                'helper' = read.csv('original-sources/helpers/county_isd_long.csv')),
           'tableau/district_school_reopening.xlsx',
           format_headers = FALSE)
```

# COUNTY LEVEL

## healthdata gov


```{r}
# page = read_html('https://beta.healthdata.gov/National/COVID-19-Community-Profile-Report/gqxm-d9w9')
# files = page %>%
#   html_nodes('script') %>%
#   str_detect('.xlsx')
# 
#   [which()] %>%
#   html_text() %>%
#   gsub('\n    var initialState =\n      ', '', .) %>%
#   gsub('\n   ;\n  ', '', .)
# # 
# newest_file = fromJSON(file_text)$view$attachments %>%
#   filter(href %>% str_detect('.xlsx')) %>%
#   dplyr::select(href) %>%
#   unlist() %>%
#   .[length(.)]
# # 
# file_date = as.Date(str_match(newest_file, '(Report )(.*)(_)')[3], '%Y%m%d')
# file_url = (paste0('https://beta.healthdata.gov', newest_file))
# 
# download.file(file_url,
#               paste0('original-sources/historical/healthdata/healthdata_', file_date, '.xlsx'),
#               mode = 'wb')
# 
# 

# # TODO: COMBINE & CLEAN FILE

```



## Google mobility

```{r}
# fread for faster processing
mobility_data = fread('https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv')
```

```{r}
# filter mobility
mobility_texas = subset(mobility_data, sub_region_1 == 'Texas')

# drop cols
mobility_texas = mobility_texas[, -c(1:3, 5:7)]

# fix colnames
colnames(mobility_texas) = c('County', 'Date', 'Retail_Recreation', 'Grocery_Pharmacy',
                             'Parks', 'Transit', 'Workplaces', 'Residential')

# Add name for blank county cells & drop 'county' suffix
mobility_texas$County = sub('^$', 'Unallocated', mobility_texas$County)
mobility_texas$County = gsub(' County', '', mobility_texas$County)

# drop blank cells
mobility_texas = subset(mobility_texas, County != 'Unallocated')

#fix types
mobility_texas$County = as.factor(mobility_texas$County)
mobility_texas$Date = as.Date(mobility_texas$Date)
```

## cases

```{r}
# download xlsx as tempfile and load using readxl
case_url = 'http://dshs.texas.gov/coronavirus/TexasCOVID19DailyCountyCaseCountData.xlsx'
temp = tempfile()
download.file(case_url, temp, mode = 'wb') 
DSHS_cases_time = data.frame(read_excel(temp, sheet = 1))

# fix colnames & select correct rows
colnames(DSHS_cases_time) = DSHS_cases_time[2, ]
DSHS_cases_time = DSHS_cases_time[3:(nrow(DSHS_cases_time) - 11), ]
colnames(DSHS_cases_time)[1] = 'County'

# convert wide data to long
DSHS_cases_long = reshape::melt(DSHS_cases_time, id = c('County'))
colnames(DSHS_cases_long) = c('County', 'Date', 'Cases_Cumulative')

# fix dates (remove linebreaks and return characters from column, then interpret as %m-%d)
DSHS_cases_long$Date = as.Date(gsub('Cases|\r|\r|\n', '', DSHS_cases_long$Date), format = '%m-%d')

# force as integer -> coerce -- to NA
# if a date is missing, fill using previous cumulative cases
# calculate daily cases by padding the initial cumulative value, then reporting the difference between successive values by county
DSHS_cases_long$Cases_Cumulative = as.integer(as.character(DSHS_cases_long$Cases_Cumulative))

DSHS_cases_long = 
  DSHS_cases_long %>%
    group_by(County) %>%
    tidyr::complete(Date = seq.Date(min(Date), date_out, by="day")) %>%
    mutate(Cases_Cumulative_NA = Cases_Cumulative) %>% 
    tidyr::fill(Cases_Cumulative, .direction = "down")
    # mutate(Cases_Daily = c(Cases_Cumulative[1], diff(Cases_Cumulative)))
```

## deaths

```{r}
death_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID19DailyCountyFatalityCountData.xlsx'
temp = tempfile()
download.file(death_url, temp, mode = 'wb') 
DSHS_deaths_time = data.frame(readxl::read_excel(temp, sheet = 1))

# fix colnames
colnames(DSHS_deaths_time) = DSHS_deaths_time[2, ]
last_row = grep('Zavala|ZAVALA', DSHS_deaths_time$County)

DSHS_deaths_time = DSHS_deaths_time[3:last_row, ]
colnames(DSHS_deaths_time)[1] = 'County'

colnames(DSHS_deaths_time) = trimws(gsub('Fatalities', '', colnames(DSHS_deaths_time)))

dates_raw = colnames(DSHS_deaths_time)[2:length(colnames(DSHS_deaths_time))]

# TODO: CHECK on 12/2, 12/3 etc. for update 
# temp fix DSHS typo 12/30 -> 11/30
dates_raw = gsub('12-30', '11-30', dates_raw)

dates = format(as.Date(dates_raw, '%m-%d'), '%Y-%m-%d')

# 11/13: DSHS reverted 11/12 change to date format (addressed below)
# dates_raw = colnames(DSHS_deaths_time)[2:length(colnames(DSHS_deaths_time))]
# dates_is_num = which(!is.na(as.numeric(dates_raw)))
# dates_num = as.Date(as.numeric(dates_raw[dates_is_num]), origin = '1899-12-30')
# dates_char = as.Date(dates_raw[-dates_is_num], '%m/%d/%Y')
# dates = sort(c(dates_num, dates_char))


# replace date values
colnames(DSHS_deaths_time)[2:ncol(DSHS_deaths_time)] = dates
# melt
DSHS_deaths_long = reshape2::melt(DSHS_deaths_time, id = c('County'))
colnames(DSHS_deaths_long) = c('County', 'Date', 'Deaths_Cumulative')

# fix most county names (ex: HARRIS -> Harris)
DSHS_deaths_long$County = stringr::str_to_title(DSHS_deaths_long$County)

# manually fix exceptions using DSHS_cases_long as dictionary
mismatched_counties = setdiff(unique(DSHS_deaths_long$County), unique(DSHS_cases_long$County))
DSHS_deaths_long$County = gsub('De Witt', 'DeWitt', DSHS_deaths_long$County)
DSHS_deaths_long$County = gsub('Dewitt', 'DeWitt', DSHS_deaths_long$County)
DSHS_deaths_long$County = gsub('Mcculloch', 'McCulloch', DSHS_deaths_long$County)
DSHS_deaths_long$County = gsub('Mclennan', 'McLennan', DSHS_deaths_long$County)
DSHS_deaths_long$County = gsub('Mcmullen', 'McMullen', DSHS_deaths_long$County)

# check again - 0 indicates all fixed
length(setdiff(unique(DSHS_deaths_long$County), unique(DSHS_cases_long$County)))

# calculate daily deaths & fill cumulative deaths
DSHS_deaths_long$Deaths_Cumulative = as.integer(as.character(DSHS_deaths_long$Deaths_Cumulative))

# date converted from date to factor during wide -> long reformat
DSHS_deaths_long = 
  DSHS_deaths_long %>%
  mutate(Date = as.Date(Date)) %>%
  # mutate(Date = as.Date(as.numeric(Date), origin = '2020-03-06')) %>%
  group_by(County) %>%
  tidyr::complete(Date = seq.Date(min(Date), date_out, by="day")) %>%
  mutate(Deaths_Cumulative_NA = Deaths_Cumulative) %>%
  tidyr::fill(Deaths_Cumulative, .direction = "down") %>%
  mutate(Deaths_Daily = c(Deaths_Cumulative[1], diff(Deaths_Cumulative))) %>% 
  ungroup()
```

## testing

```{r}
# LEGACY
legacy_tests_long = read.csv('original-sources/historical/testing/DSHS_county_tests_legacy_cleaned.csv') %>% 
  mutate(Date = as.Date(Date))

# NEW
test_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID-19CumulativeTestsbyCounty.xlsx'
temp = tempfile()
download.file(test_url, temp, mode = 'wb')

new_tests = data.frame(readxl::read_excel(temp, sheet = 1))
new_tests_long = new_tests %>% 
  setNames(new_tests[1, ]) %>% 
  filter(str_detect(County, 'County|Unknown|Total', negate = TRUE)) %>% 
  reshape::melt(id = 'County') %>% 
  setNames(c('County', 'Date', 'Tests_Cumulative')) %>% 
  mutate(Date = as.Date(as.integer(Date), '2020-09-12'))

# MERGE
merged_tests = rbind(legacy_tests_long, new_tests_long)

DSHS_tests_long = merged_tests %>%
  group_by(County) %>% 
  tidyr::complete(Date = seq.Date(min(Date), date_out, by="day")) %>%
  mutate(Tests_Cumulative_NA = Tests_Cumulative) %>%
  tidyr::fill(Tests_Cumulative, .direction = "down") %>%
  mutate(Tests_Daily = c(Tests_Cumulative[1], diff(Tests_Cumulative)))
```

## new cases

```{r}
Date_Parser = function(Date) { 
  #Matches 2 or 4 digits, separator, 2 or 4 digits, optional separator, optional 2 or 4 digits
  # Coerces all common separators to "-" 
  date_regex = '(\\d{2}|\\d{4})(\\.|\\-|\\/)(\\d{2}|\\d{4})?(\\.|\\-|\\/)?(\\d{2}|\\d{4})'
  clean_dates = str_extract(Date, date_regex) %>% str_replace_all(., '\\/|\\.', '\\-')

  # TODO: add more as needed
  md_dates = which(!is.na(as.Date(clean_dates, format = '%m-%d')))
  clean_dates[md_dates] = format(as.Date(clean_dates[md_dates], '%m-%d'), '%Y-%m-%d')
  
  dates_out = as.Date(clean_dates)
  return(dates_out)
}


# 10/23 - file includes all dates now
# get new cases as excel file
new_case_url ='https://dshs.texas.gov/coronavirus/TexasCOVID-19NewCasesOverTimebyCounty.xlsx'
temp = tempfile()
download.file(new_case_url, temp, mode = 'wb') 


# MANUALLY ADD MONTGOMERY CASES PER MISTI WILLINGHAM (11/20)
# 11/1 - 11/20 [manual entry] | 11/21 DSHS value
montgomery_new_cases = c(32, 61, 78, 110, 98, 124, 78, 77, 144, 97, 146, 133, 124, 103, 90, 109, 33, 3, 330, 162, 484)
montgomery_case_dates = seq(as.Date('2020-11-01'), as.Date('2020-11-21'), by = 'day')

DSHS_new_cases_long = data.frame(readxl::read_excel(temp, sheet = 1, skip = 2)) %>%
  slice(1:254) %>%
  reshape2::melt('County') %>% 
  setNames(c('County', 'Date', 'Cases_Daily')) %>%
  mutate(Date = gsub('New Cases ', '', Date)) %>%
  mutate(Date = Date_Parser(Date)) %>% 
  mutate(Cases_Daily = ifelse(Cases_Daily < 0, 0, Cases_Daily)) %>%
  mutate(Cases_Daily = replace(Cases_Daily, 
                               County == 'Montgomery' & Date %in% montgomery_case_dates,
                               montgomery_new_cases)) %>%
  # rbind(., new_cases_archive) %>% 
  arrange(Date, County)
```

## active cases

```{r}
# historic 
old_case_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID-19LegacyActiveCasesbyCounty.xlsx'
temp = tempfile()
download.file(old_case_url, temp, mode = 'wb') 
old_active_cases = data.frame(readxl::read_excel(temp, sheet = 1, skip = 2)) %>%
  slice(which(County == 'Anderson'):which(County == 'Zavala')) %>% 
  dplyr::select(-Notes)


# temp fix duplicate cols - default to .x
if (length(grep('.x', colnames(old_active_cases)) > 0)) { 
    old_active_cases = old_active_cases[, -grep('\\.y', colnames(old_active_cases))]
    colnames(old_active_cases) = gsub('\\.y', '', colnames(old_active_cases))
}


# melt
old_active_cases_long = reshape::melt(old_active_cases, id = 'County') %>% 
  setNames(c('County', 'Date', 'Active_Cases_Cumulative')) %>%
  group_by(Date) %>%
  mutate(Date = as.Date(paste(str_extract_all(Date, '\\d+')[[1]], collapse = ' '), '%m%d')) %>% 
  ungroup() %>%
  group_by(County) %>% 
  distinct() %>%
  mutate(Active_Cases_Cumulative = as.integer(as.character(Active_Cases_Cumulative))) %>%
  tidyr::complete(Date = seq.Date(min(Date), as.Date('2020-12-10'), by="day")) %>%
  mutate(Active_Cases_Cumulative_NA = Active_Cases_Cumulative) %>%
  tidyr::fill(Active_Cases_Cumulative, .direction = "down") %>%
  mutate(Active_Cases_Daily = c(Active_Cases_Cumulative[1], diff(Active_Cases_Cumulative))) %>%
  filter(!is.na(County))



# current
new_case_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID-19ActiveCaseDatabyCounty.xlsx'
temp = tempfile()
download.file(new_case_url, temp, mode = 'wb') 
new_active_cases = data.frame(readxl::read_excel(temp, sheet = 1, skip = 2)) %>%
  slice(which(County == 'Anderson'):which(County == 'Zavala')) %>% 
  dplyr::select(-Notes) %>% 
  select_if(function(x) all(!is.na(x)))

# melt
new_active_cases_long = reshape::melt(new_active_cases, id = 'County') %>% 
  setNames(c('County', 'Date', 'Active_Cases_Cumulative')) %>%
  group_by(Date) %>%
  mutate(Date = as.Date(paste(str_extract_all(Date, '\\d+')[[1]], collapse = ' '), '%m%d')) %>% 
  ungroup() %>%
  group_by(County) %>% 
  distinct() %>%
  mutate(Active_Cases_Cumulative = as.integer(as.character(Active_Cases_Cumulative))) %>%
  tidyr::complete(Date = seq.Date(min(Date), date_out, by="day")) %>%
  mutate(Active_Cases_Cumulative_NA = Active_Cases_Cumulative) %>%
  tidyr::fill(Active_Cases_Cumulative, .direction = "down") %>%
  mutate(Active_Cases_Daily = c(Active_Cases_Cumulative[1], diff(Active_Cases_Cumulative))) %>%
  filter(!is.na(County))



DSHS_active_cases_long = rbind(old_active_cases_long, new_active_cases_long) %>% 
  arrange(Date, County)
```

## childcare

```{r}
# runtime bottleneck: switch pull to once per week (all data is stored in one file)
# Get_Childcare_Data = function(date_out) { 
#   if (date_out %in% seq(as.Date('2020-10-18'), length.out = 52, by = 'week')) { 
#     temp = tempfile()
#     download.file('https://apps.hhs.texas.gov/documents/CCR/texas-child-care-facility-based-covid-status.xls',
#               temp, mode = 'wb') 
#     county_childcare = read_excel_allsheets(temp)
#     county_childcare_df = rbindlist(lapply(county_childcare, Clean_Childcare))
#     write.csv(county_childcare_df, 'original-sources/county_childcare.csv', row.names = FALSE)
#   } else { 
#     county_childcare_df = read.csv('original-sources/county_childcare.csv') %>% 
#       mutate(Date = as.Date(Date))
#     }
#   return(county_childcare_df)
# }    
# 
# 
# Clean_Childcare = function(df, data_type) { 
#   date = as.Date(gsub('Data current as of ', '', colnames(df)[1]), '%m/%d/%y')
#   colnames(df) = df[3, ]
#   
#   df = df[4:(nrow(df)-1), c(1, 7:10)]
#   
#   grouped_df = df %>% 
#     filter(County != 'Licensed Child Care Center' & County != 'School-Age Program') %>%
#     mutate(County = str_to_title(County)) %>% 
#     mutate(County =  gsub('Deafsmith', 'Deaf Smith', County)) %>% 
#     mutate(County =  gsub('Dewitt', 'DeWitt', County)) %>% 
#     mutate(County =  gsub('Mcculloch', 'McCulloch', County)) %>% 
#     mutate(County =  gsub('Mclennan', 'McLennan', County)) %>% 
#     group_by(County) %>%
#     dplyr::select(2:5) %>%
#     mutate_all(as.numeric) %>% 
#     summarise_all(sum) %>% 
#     setNames(c('County', 'Childcare_Child_Cases_Daily', 'Childcare_Employee_Cases_Daily',
#                'Childcare_Child_Cases_Cumulative', 'Childcare_Employee_Cases_Cumulative')) %>% 
#     mutate(Date = date)
#   return(grouped_df)
# }

# county_childcare_df = Get_Childcare_Data(date_out)
```

## computed

```{r}
# county_school_counts = DSHS_school_df %>% 
  # group_by(Date, County) %>% 
  # summarize_at(vars(8:ncol(.)-2), funs(sum), na.rm = TRUE)
```


### merge

```{r}
# combine DSHS sources using merge helper function
# https://www.musgraveanalytics.com/blog/2018/2/12/how-to-merge-multiple-data-frames-using-base-r

county_counts = Reduce(function(x, y) merge(x, y, by = c('Date', 'County'), all=TRUE),
       list(DSHS_cases_long, DSHS_new_cases_long, DSHS_deaths_long, DSHS_tests_long,
            DSHS_active_cases_long))
```


## merge

```{r}
# DSHS pop - removed by DSHS 07/30 - defaulting to previous file
dshs_pops = unique(read.csv('https://raw.githubusercontent.com/jeffbrennan/COVID-19/d03d476f7fb060dfd2e1a600a6a1e449df0ab8df/original-sources/DSHS_county_cases.csv')[, c('County', 'Population')])
colnames(dshs_pops) = c('County', 'Population_DSHS')

merged_dshs = Reduce(function(x, y) merge(x, y, by = 'County', all = TRUE),
                       list(county_counts, tsa_long_complete, dshs_pops, county_classifications))

# add TSA, PHR & HHSC combination
merged_dshs$TSA_Combined = paste0(merged_dshs$TSA, ' - ', merged_dshs$TSA_Name)
merged_dshs$PHR_Combined = paste0(merged_dshs$PHR, ' - ', merged_dshs$PHR_Name)
# merged_dshs$HHSC_Combined = paste0(merged_dshs$HHSC, ' - ', merged_dshs$HHSC_Name)

merged_county = as.data.frame(merge(merged_dshs, mobility_texas,
                                    by = c('Date', 'County'), all = TRUE)) %>% 
  filter(!is.na(County) & County != 'Unknown')

# fix types
merged_county$County = as.factor(merged_county$County)
merged_county$Population_DSHS = as.numeric(merged_county$Population_DSHS)

# keep only relevant dates (previous dates include google mobility only)
merged_county = merged_county %>% 
  filter(Date >= as.Date('2020-03-04') & !is.na(County)) %>% 
  distinct() 
```

```{r}
# data dump investigation - move to diagnostics 
daily_tests = merged_county %>% 
  dplyr::select(Date, County, Tests_Cumulative, Tests_Daily) %>% 
  filter(County == 'Harris' & Date > as.Date('2020-09-01'))

ggplot(daily_tests, aes(x = Date, y = Tests_Daily)) + 
  geom_point() +
  geom_line(color = 'red') +
  labs(title = 'Daily Tests since 9/1 in Harris County') + 
  theme_pubr()


# tpr calculation
merged_county %>%
  dplyr::select(Date, County, Cases_Cumulative, Cases_Daily, Tests_Daily) %>%
  group_by(County) %>% 
  mutate(Cases_Cumulative_daily = as.numeric(c(Cases_Cumulative[1], diff(Cases_Cumulative)))) %>%
  filter(County == 'Harris' & Date > Sys.Date() - 14) %>% 
  summarize(TPR_new_cases = sum(Cases_Daily) / sum(Tests_Daily),
            TPR_cumulative_cases = sum(Cases_Cumulative_daily) / sum(Tests_Daily))

```


## TPR 

```{r}
Get_TPR = function() { 
  page = read_html('https://data.cms.gov/stories/s/q5r5-gjyu')
  tpr_url = page %>% html_nodes('a') %>% html_attr('href') %>%
    .[grepl('download', .)] %>% .[1] %>% gsub('%2F', '/', .)
  
  # temp dir needed for unzip to keep directory clean
  temp_dir = tempdir()
  temp_file = tempfile()
  download.file(tpr_url, temp_file, mode = 'wb')
  tpr_df = read_excel(unzip(temp_file, exdir = temp_dir)[[1]])
  tpr_date = as.Date(str_match(tpr_df[2,2], '-(.*)')[2], '%B %d')
  print(tpr_date)
  
  tpr_out = tpr_df %>% 
    setNames(.[which(tpr_df[, 1] == 'County'), ]) %>% 
    filter(State == 'TX') %>% 
    mutate(County = gsub(' County, TX', '', County)) %>% 
    dplyr::select(1, 7, 9) %>% 
    mutate_at(c(2,3), as.numeric) %>% 
    setNames(c('County', 'Tests', 'TPR_CMS')) %>% 
    mutate(Date = tpr_date) %>% 
    dplyr::select(County, Date, Tests, TPR_CMS)

  write.csv(tpr_out, paste0('original-sources/historical/TPR/TPR_', tpr_date, '.csv'),
            row.names = FALSE) 
  }
  
# TODO: find out which day the new file is added
TPR_dates = seq(as.Date('2020-08-19'), by = 'week', length.out = 52)
# if (date_out %in% (TPR_dates + 5)) {Get_TPR()}

Get_TPR()

tpr_df = rbindlist(
         lapply(list.files('original-sources/historical/TPR', full.names = TRUE), read.csv),
         fill = TRUE) %>% 
         mutate(Date = as.Date(Date))

tpr_cases = merged_county %>% 
  dplyr::select(County, Date, Cases_Daily, Population_DSHS) %>%
  filter(Date >= as.Date(min(TPR_dates)) - 13 & Date <= max(tpr_df$Date)) %>%
  group_by(County) %>%
  mutate(Cases_100K_7Day_MA = (rollmean(Cases_Daily, k = 7, align = 'right',
                                        na.pad = TRUE, na.rm = TRUE)
                               / Population_DSHS) * 100000) %>%
  mutate(Cases_100K_14Day_MA = (rollmean(Cases_Daily, k = 14, align = 'right',
                                         na.rm = TRUE, na.pad = TRUE)
                                / Population_DSHS) * 100000) %>% 
  filter(Date %in% TPR_dates) %>% 
  dplyr::select(-Cases_Daily, -Population_DSHS, -Cases_100K_14Day_MA)

tpr_out = tpr_df %>% inner_join(tpr_cases, by = c('County', 'Date')) %>% arrange(County, Date)
write.csv(tpr_out, 'tableau/county_TPR.csv', row.names = FALSE)
```

# TSA LEVEL

## Computed

```{r}
# longitudinal counts (sum)
DSHS_tsa_counts =
    merged_county %>%
    group_by(Date, TSA, TSA_Name) %>% 
    summarize_at(vars(Cases_Cumulative, Cases_Daily,
                      Deaths_Cumulative, Deaths_Daily,
                      Tests_Cumulative, Tests_Daily,
                      Active_Cases_Cumulative, Active_Cases_Daily),
                 funs(sum))

# static pop counts (sum)
DSHS_tsa_pops = 
  subset(merged_county, Date == '2020-03-04') %>%
  group_by(TSA) %>%
  summarize_at(vars(Population_DSHS),
               funs(sum))

# longitudinal google data (mean)
DSHS_tsa_google = 
  merged_county %>%
  group_by(Date, TSA, TSA_Name) %>%
  summarize_at(vars(Retail_Recreation, Grocery_Pharmacy,
                    Parks, Transit,
                    Workplaces, Residential),
               funs(weighted.mean(., Population_DSHS)), na.rm = TRUE)

DSHS_tsa = merge(DSHS_tsa_counts, DSHS_tsa_google, by = c('Date', 'TSA', 'TSA_Name'))
DSHS_tsa = merge(DSHS_tsa, DSHS_tsa_pops, by = 'TSA', all = TRUE)
```

## DSHS hospitals

```{r}
hosp_url = 'https://dshs.texas.gov/coronavirus/CombinedHospitalDataoverTimebyTSA.xlsx'
temp = tempfile()

download.file(hosp_url, temp, mode = 'wb') 
DSHS_tsa_hosp = read_excel_allsheets(temp)
 
DSHS_hosp_clean = function(df, var_name) { 
  colnames(df) = df[1, ]
  df = df[2:23, c(1, 3:ncol(df))]
  df$`TSA ID` = gsub('.', '', df$`TSA ID`, fixed = TRUE)

  # add temp fix for DSHS duplicated dates (8/8) (fixed by DSHS 8/10)
  # TODO: add dynamic handling
  if (length(grep('.x', colnames(df)) > 0)) { 
    df = df[, -grep('.x', colnames(df))]
    colnames(df) = gsub('.y', '', colnames(df))
  }

  # TODO: make date conversion function
  dates = colnames(df)[2:length(colnames(df))]
  numeric_dates = which(is.na(as.Date(dates, format = '%Y-%m-%d')))
  
  # convert from 5 digit excel numeric format
  # https://stackoverflow.com/questions/43230470/how-to-convert-excel-date-format-to-proper-date-in-r
  dates[numeric_dates] = format(as.Date(as.integer(dates[numeric_dates]), origin = '1899-12-30'))
  dates[-numeric_dates] = format(as.Date(dates[-numeric_dates]), '%Y-%m-%d')
  colnames(df)[2:length(colnames(df))] = dates

  df_long = reshape::melt(df, id = 'TSA ID')
  colnames(df_long) = c('TSA', 'Date', var_name)
  df_long$Date = as.Date(df_long$Date)
  
  if(length(which(df_long$Date == '2008-08-08')) > 0) {
    df_long$Date[which(df_long$Date == '2008-08-08')] = as.Date('2020-08-08')
  }
  
  return(df_long)
}
# 1,2,3 references the 3 sheets produced by DSHS
hosp_1 = DSHS_hosp_clean(DSHS_tsa_hosp[[1]], 'Hospitalizations_Total')
hosp_2 = DSHS_hosp_clean(DSHS_tsa_hosp[[8]], 'Hospitalizations_General')
hosp_3 = DSHS_hosp_clean(DSHS_tsa_hosp[[9]], 'Hospitalizations_ICU')
hosp_cap1 = DSHS_hosp_clean(DSHS_tsa_hosp[[4]], 'Beds_Available_Total')
hosp_cap2 = DSHS_hosp_clean(DSHS_tsa_hosp[[5]], 'Beds_Available_ICU')
hosp_cap3 = DSHS_hosp_clean(DSHS_tsa_hosp[[10]], 'Beds_Occupied_Total')
hosp_cap4 = DSHS_hosp_clean(DSHS_tsa_hosp[[11]], 'Beds_Occupied_ICU')
```

## DSHS dashboard

```{r}
# pull DSHS dashboard data using link found from inspect element -> network
DSHS_json_hosp_tsa = jsonlite::fromJSON("https://services5.arcgis.com/ACaLB9ifngzawspq/arcgis/rest/services/DSHS_COVID_Hospital_Data/FeatureServer/0/query?f=json&where=1%3D1&returnGeometry=false&spatialRel=esriSpatialRelIntersects&outFields=*&outSR=102100&resultOffset=0&resultRecordCount=25&resultType=standard&cacheHint=true")[['features']][['attributes']]

DSHS_json_hosp_tsa = DSHS_json_hosp_tsa[, c(2,5:9)]
# 
colnames(DSHS_json_hosp_tsa) = c('TSA', 'Hospital_Beds_Staffed', 'Hospital_Beds_Available',
                                 'ICU_Beds_Available', 'Ventilators_Available', 'Current_Cases')

# export today's file
write.csv(DSHS_json_hosp_tsa, paste0('original-sources/historical/hosp/tsa_hosp_', date_out, '.csv'),
          row.names = FALSE)

# read in all files
hosp_list = list.files('original-sources/historical/hosp', pattern = '*.csv', full.names = TRUE)
hosp_dates = sapply(hosp_list, function(x) str_extract(x, '\\d{4}-\\d{2}-\\d{2}'))
tsa_all_hosp = lapply(hosp_list, read.csv, fileEncoding = 'UTF-8-BOM')
names(tsa_all_hosp) = hosp_dates

tsa_combined_hosp = rbindlist(tsa_all_hosp, fill = TRUE, idcol = TRUE) %>% 
  dplyr::select(-Date) %>% 
  rename(Date = .id) %>% 
  dplyr::select(TSA, Date, Ventilators_Available)
```

## merge

```{r}
merged_tsa = Reduce(function(x, y) merge(x, y, by = c('Date', 'TSA'), all = TRUE),
                    list(DSHS_tsa, tsa_combined_hosp,
                         hosp_1, hosp_2, hosp_3,
                         hosp_cap1, hosp_cap2, hosp_cap3, hosp_cap4))

# fix types & ensure TSA values are valid
merged_tsa = merged_tsa %>% 
  mutate_at(vars(Beds_Available_Total, Beds_Available_ICU, Ventilators_Available,
                 Beds_Occupied_Total, Beds_Occupied_ICU, Hospitalizations_Total,
                 Hospitalizations_General, Hospitalizations_ICU),
            funs(as.character)) %>%
  mutate_at(vars(Beds_Available_Total, Beds_Available_ICU, Ventilators_Available,
                 Beds_Occupied_Total, Beds_Occupied_ICU, Hospitalizations_Total,
                 Hospitalizations_General, Hospitalizations_ICU),
            funs(as.integer)) %>%
  mutate(TSA_Combined = paste0(TSA, ' - ', TSA_Name)) %>%
  filter(!is.na(TSA) & !is.na(Date)) %>% 
  distinct()
```


# STATE LEVEL

## HHS facility reporting

```{r}
# read in files
hhs_fac_reporting_raw = read.csv(url('https://opendata.arcgis.com/datasets/adf5753521ed4e2199d6a4c246e08f84_0.csv'))

hhs_date = as.Date(hhs_fac_reporting_raw$last_updated)
hhs_fac_reporting_raw$Date = hhs_date
hhs_fac_reporting_raw$last_updated = NULL

# combine csv for archival
write_xlsx(list('1' = hhs_fac_reporting_raw),
           path = paste0('original-sources/historical/hhs/hhs_data_', hhs_date, '.xlsx'))

# read in archival sources
hhs_list <- paste0('original-sources/historical/hhs/',
                    list.files(path = 'original-sources/historical/hhs',
                               pattern = '*.xlsx'))

HHS_fac_clean = function(df) { 
    df$Date = format(as.Date(df$Date, origin = '1899-12-30'), '%Y-%m-%d')
    
    if (df$Date[1] <= as.Date('2020-08-03')) {
    
    clean_df = df %>% 
      filter(state_name == 'Texas') %>% 
      dplyr::select(-c('ï..OBJECTID', 'Shape__Area', 'Shape__Length', 'state_fips',
                'state_abbr', 'hhs_region', 'state_name'))
    
    colnames(clean_df)[1:3] = c('reporting_hospitals', 'total_hospitals', 'percent_reporting')
    colnames(clean_df)[1:3] = paste0('HHS_', colnames(clean_df)[1:3])
    
    # drop date from colnames (will likely change over time)
    colnames(clean_df) = gsub('_*\\d', '', colnames(clean_df))
    
  } else { 
    clean_df = df %>% 
      filter(state_name == 'Texas') %>% 
      dplyr::select(-c('ï..OBJECTID', 'state_name'))
    
    colnames(clean_df)[1:3] = paste0('HHS_', colnames(clean_df)[1:3])
    }
  return(clean_df)
}

# 1st lapply: read excel sheets
# 2nd: clean all of them
# rbind cleaned sheets together
HHS_fac_reporting <- rbindlist(lapply(lapply(hhs_list, read_xlsx, sheet = 1), HHS_fac_clean))
```

## HHS hospitalization

```{r}
HHS_hosp_clean = function(df, data_type) { 
  clean_df = df %>% 
    filter(state == 'TX') %>% 
    dplyr::select(-c('state'))
  
  # append type of data to colnames
  colnames(clean_df)[c(3:4, 6:7)] = paste0(data_type, colnames(clean_df[c(3:4, 6:7)]))
  colnames(clean_df) = paste0('HHS_', colnames(clean_df))
  
  clean_df$Date = as.Date(clean_df$HHS_collection_date)
  clean_df$HHS_collection_date = NULL

  return(clean_df)
}
# timeseries files 
HHS_all_inpatient = 
  HHS_hosp_clean(
    read.csv(url('https://healthdata.gov/sites/default/files/reported_inpatient_all_20200720_0537.csv')),
    'All.Inpatient.')

HHS_covid_inpatient = 
  HHS_hosp_clean(
    read.csv(url('https://healthdata.gov/sites/default/files/inpatient_covid_final_20200720_0537.csv')),
    'COVID.Inpatient.')

HHS_all_icu = 
  HHS_hosp_clean(
    read.csv(url('https://healthdata.gov/sites/default/files/icu_final_20200720_0537.csv')),
    'All.ICU.')
```

### merge

```{r}
# merge together for combination with other state data
hhs_df = Reduce(function(x, y) merge(x, y, by = 'Date', all = TRUE),
                list(HHS_all_inpatient, HHS_covid_inpatient, HHS_all_icu, HHS_fac_reporting))

```

## DSHS (time series)

```{r}
state_url = 'https://www.dshs.state.tx.us/coronavirus/TexasCOVID19CaseCountData.xlsx'
temp = tempfile()
download.file(state_url, temp, mode = 'wb')
dshs_header = names(read_excel(temp, sheet = 1)[1])

current_year =  substr(Sys.Date(), 1, 4)
dshs_date = paste0(current_year, '/',  str_extract_all(dshs_header, '\\d*\\/\\d*')[[1]])
dshs_date = format(as.Date(dshs_date), '%Y_%m_%d')

# save state level file to historical database for longitudinal demo
download.file(state_url, paste0('original-sources/historical/state/dshs_',
                                dshs_date, '.xlsx'), mode = 'wb')

DSHS_state = read_excel_allsheets(temp)
```

```{r}
DSHS_tests = DSHS_state[[4]]

# # LAB TESTS
# select lab tests & remove rows where all rows are NA
DSHS_lab_tests = DSHS_tests[2:nrow(DSHS_tests), 1:4] %>%
  setNames(c('Date', 'LAB_Tests_Daily', 'LAB_Positive_Tests_Daily','LAB_Positivity_Rate')) %>%
  filter(rowSums(is.na(.)) < ncol(.)) %>%
  mutate(Date = as.Date(as.integer(Date), origin = '1899-12-30'))
#
# # SPECIMEN TESTS
DSHS_specimen_tests = DSHS_tests[2:nrow(DSHS_tests), c(6,7,9,11)] %>%
  setNames(c('Date', 'SPECIMEN_Tests_Daily','SPECIMEN_Positive_Tests_Daily',
             'SPECIMEN_Positivity_Rate')) %>%
  filter(rowSums(is.na(.)) < ncol(.)) %>%
  mutate(Date = as.Date(as.integer(Date), origin = '1899-12-30'))
```

```{r}
# selects covid hospitalization time series, drops footnote, formates date and sets colnames
DSHS_hospitalizations = DSHS_state[[9]][, c(2:3)] %>%
  filter(rowSums(is.na(.)) < ncol(.)) %>% 
  setNames(c('Date', 'Hospitalizations_Total')) %>%
  mutate(Date = as.Date(Date))
```

```{r}
# merge tests & hospitalizations
DSHS_state_time = Reduce(function(x, y) merge(x, y, by = c('Date')),
list(DSHS_specimen_tests, DSHS_lab_tests, DSHS_hospitalizations))

# DSHS_state_time = DSHS_hospitalizations
```

## DSHS (day counts)

```{r}
# avoids duplication of results between sheets
DSHS_state_day = data.frame(Molecular_Tests_Total = DSHS_state[[5]][1,2],
                            Antigen_Tests_Total = DSHS_state[[7]][1,2],
                            Antigen_Tests_Positive = DSHS_state[[7]][2,2],
                            Antibody_Tests_Total = DSHS_state[[6]][1,2],
                            Antibody_tests_Positive = DSHS_state[[6]][2,2],
                            Hospital_Bed_Total = DSHS_state[[8]][2,2],
                            Hospital_Bed_Available = DSHS_state[[8]][4,2],
                            ICU_Bed_Available = DSHS_state[[8]][5,2],
                            Ventilator_Available = DSHS_state[[8]][6,2])
```

## DSHS Demographics

```{r}
# TODO: refactor (clean once and append to the cleaned files) - primary script bottleneck
# read in all files
demo.list <- paste0('original-sources/historical/state/',
                    list.files(path = 'original-sources/historical/state',
                               pattern = '*.xlsx'))

DSHS_case_age <- c(lapply(demo.list[c(1:66, 104:112)], read_excel, sheet = 9),
                   lapply(demo.list[67:103], read_excel, sheet = 10),
                   # lapply(demo.list[113:length(demo.list)], read_excel, sheet = 8))
                   lapply(demo.list[113], read_excel, sheet = 8))

DSHS_case_gender <- c(lapply(demo.list[c(1:66, 104:112)], read_excel, sheet = 10),
                      lapply(demo.list[67:103], read_excel, sheet = 11),
                      # lapply(demo.list[113:length(demo.list)], read_excel, sheet = 9))
                      lapply(demo.list[113], read_excel, sheet = 9))


DSHS_case_race <- c(lapply(demo.list[c(1:66, 104:112)], read_excel, sheet = 11),
                    lapply(demo.list[67:103], read_excel, sheet = 12),
                    # lapply(demo.list[113:length(demo.list)], read_excel, sheet = 10))
                    lapply(demo.list[113], read_excel, sheet = 10))


DSHS_death_age <- c(lapply(demo.list[c(1:66, 104:112)], read_excel, sheet = 12),
                    lapply(demo.list[67:103], read_excel, sheet = 13),
                    # lapply(demo.list[113:length(demo.list)], read_excel, sheet = 11))
                    lapply(demo.list[113], read_excel, sheet = 11))


DSHS_death_gender <- c(lapply(demo.list[c(1:66, 104:112)], read_excel, sheet = 13),
                       lapply(demo.list[67:103], read_excel, sheet = 14),
                       # lapply(demo.list[113:length(demo.list)], read_excel, sheet = 12))
                       lapply(demo.list[113], read_excel, sheet = 12))


DSHS_death_race <- c(lapply(demo.list[c(1:66, 104:112)], read_excel, sheet = 14),
                          lapply(demo.list[67:103], read_excel, sheet = 15),
                          # lapply(demo.list[113:length(demo.list)], read_excel, sheet = 13))
                          lapply(demo.list[113], read_excel, sheet = 13))


DSHS_demo_clean = function(df, data_type) {
  # search for updated date (digit "/" digit)
  date = str_extract_all(colnames(df)[1], '\\d+\\/\\d+')[[1]]

  df = df[2:(nrow(df) - 4), c(1:2)]
  colnames(df) = c('Group', 'var_Cumulative')

  # remove any rows containing total
  total_check = which(df[, 'Group'] == 'Total')
  if (length(total_check) != 0) {df = df[-which(df[, 'Group'] == 'Total'), ]}

  df = df %>%
    mutate(Date = as.Date(paste0('2020/', date))) %>%
    mutate(Group_Type = data_type[1]) %>%
    mutate(var_Cumulative = as.numeric(var_Cumulative)) %>%
    group_by(Date) %>%
    mutate(var_PCT = var_Cumulative / sum(var_Cumulative)) %>%
    na.omit()

  colnames(df) = gsub('var', data_type[2], colnames(df))
  return(df)
}

DSHS_case_age_df = rbindlist(lapply(DSHS_case_age, DSHS_demo_clean, data_type = c('Age', 'Cases')))
DSHS_case_gender_df = rbindlist(lapply(DSHS_case_gender, DSHS_demo_clean, data_type = c('Gender', 'Cases')))
DSHS_case_race_df = rbindlist(lapply(DSHS_case_race, DSHS_demo_clean,  data_type = c('Race', 'Cases')))

DSHS_death_age_df = rbindlist(lapply(DSHS_death_age, DSHS_demo_clean, data_type = c('Age', 'Deaths')))
DSHS_death_gender_df = rbindlist(lapply(DSHS_death_gender, DSHS_demo_clean, data_type = c('Gender', 'Deaths')))
DSHS_death_race_df = rbindlist(lapply(DSHS_death_race, DSHS_demo_clean, data_type = c('Race', 'Deaths')))


# combine case and death cols
DSHS_age_df = merge(DSHS_case_age_df, DSHS_death_age_df, by = c('Date', 'Group_Type',  'Group'))
DSHS_gender_df = merge(DSHS_case_gender_df, DSHS_death_gender_df, by = c('Date', 'Group_Type', 'Group'))
DSHS_race_df = merge(DSHS_case_race_df, DSHS_death_race_df, by = c('Date', 'Group_Type', 'Group'))

demo_stack = rbind(DSHS_age_df, DSHS_gender_df, DSHS_race_df) %>% arrange(Date)
```

## POST 9/25 DEMOGRAPHIC SCRAPING

```{r}
Clean_Demographics = function(index) {
  group_conversion =  c('Age', 'Gender', 'Race')
  base_url = 'https://services5.arcgis.com/ACaLB9ifngzawspq/arcgis/rest/services/DSHS_COVID19_Cases_Service/FeatureServer/'
  
  case_df = fromJSON(paste0(base_url, index, '/query?f=json&where=1%3D1&outFields=*'))[['features']][['attributes']][, c(2:3)] %>% 
    setNames(c('Group', 'Cases_Cumulative')) %>%
    mutate(Date = date_out) %>% 
    mutate(Group_Type = group_conversion[index]) %>% 
    group_by(Date) %>% 
    mutate(Cases_PCT = Cases_Cumulative / sum(Cases_Cumulative)) %>%
    dplyr::select(Date, Group_Type, Group, Cases_Cumulative, Cases_PCT)

  death_df = fromJSON(paste0(base_url, index + 3, '/query?f=json&where=1%3D1&outFields=*'))[['features']][['attributes']][, c(2:3)] %>% 
    setNames(c('Group', 'Deaths_Cumulative')) %>%
    mutate(Date = date_out) %>% 
    mutate(Group_Type = group_conversion[index]) %>% 
    group_by(Date) %>% 
    mutate(Deaths_PCT = Deaths_Cumulative / sum(Deaths_Cumulative)) %>%
    dplyr::select(Date, Group_Type, Group, Deaths_Cumulative, Deaths_PCT)
  
  clean_df = merge(case_df, death_df, by = c('Date', 'Group_Type', 'Group'))
  return(clean_df)
}

demo_releases = seq(as.Date('2020-09-25'), by = 'week', length = 52)

# only run weekly
if (date_out %in% demo_releases) {
  daily_demo_stack = rbindlist(lapply(seq(1,3), Clean_Demographics))
  write.csv(daily_demo_stack, paste0('original-sources/historical/demo-json/dshs_', date_out, '.csv'), row.names = FALSE)
}

```

### merge

```{r}
daily_demo_stack_all = rbindlist(lapply(list.files('original-sources/historical/demo-json/',
                                                   full.names = TRUE), read.csv)) %>% 
  mutate(Date = as.Date(Date)) %>% 
  distinct(Cases_Cumulative, Cases_PCT, Deaths_Cumulative, Deaths_PCT, .keep_all = TRUE)

demo_stacked_combined = rbind(demo_stack, daily_demo_stack_all) %>% 
  group_by(Group_Type, Group) %>% 
  mutate(Cases_Daily = as.numeric(c(Cases_Cumulative[1], diff(Cases_Cumulative)))) %>%
  mutate(Cases_Daily_NO_NEGATIVE = ifelse(Cases_Daily < 0, 0, Cases_Daily)) %>%
  mutate(Deaths_Daily = as.numeric(c(Deaths_Cumulative[1], diff(Deaths_Cumulative)))) %>%
  mutate(Deaths_Daily_NO_NEGATIVE = ifelse(Deaths_Daily < 0, 0, Deaths_Daily)) %>% 
  dplyr::select(Date, Group_Type, Group, Cases_Cumulative, Cases_PCT, Cases_Daily, Cases_Daily_NO_NEGATIVE, everything())
```

## Computed

```{r}
state_counts =
    merged_county %>%
    group_by(Date) %>%
    summarize_at(vars(Cases_Cumulative, Cases_Daily,
                      Deaths_Cumulative, Deaths_Daily,
                      Tests_Cumulative, Tests_Daily,
                      Active_Cases_Cumulative, Active_Cases_Daily),
                 funs(sum(., na.rm = TRUE)))

state_pops = 
  subset(merged_county, Date == '2020-03-04') %>%
  group_by(Date) %>%
  summarize_at(vars(Population_DSHS),
               funs(sum))

state_google = 
  merged_county %>%
  group_by(Date) %>%
  summarize_at(vars(Retail_Recreation, Grocery_Pharmacy,
                    Parks, Transit,
                    Workplaces, Residential),
               funs(weighted.mean(., Population_DSHS)), na.rm = TRUE)

state_hosp_detail = 
  merged_tsa %>% 
  group_by(Date) %>%
  summarize_at(vars(Hospitalizations_Total, Hospitalizations_General, Hospitalizations_ICU,
                    Beds_Available_Total, Beds_Available_ICU,  Beds_Occupied_Total, Beds_Occupied_ICU,
                    Ventilators_Available),
               funs(sum))
```

## merge

```{r}
merged_state = Reduce(function(x, y) merge(x, y, by = c('Date'), all=TRUE),
       list(state_counts, state_google, DSHS_state_time, state_hosp_detail, hhs_df))

merged_state$Population_DSHS = state_pops$Population_DSHS

merged_state = merged_state %>% 
  filter(Date >= as.Date('2020-03-04')) %>% 
  distinct()
```

# OUTPUT

## Schools

```{r}
write.csv(nyt_schools, file = 'original-sources/historical/nyt/nyt_colleges.csv', row.names = F)
```

## City

```{r}
# write.csv(merged_city, 'tableau/city_pops.csv', row.names = F)
```

## County

```{r}
merged_county_out = merged_county %>% 
  dplyr::select(-c(Cases_Cumulative_NA, Deaths_Cumulative_NA,
                   Tests_Cumulative_NA, Active_Cases_Cumulative_NA))

# write.csv(merged_county_out, file = 'combined-datasets/county.csv', row.names = F)
write.csv(merged_county_out, 'tableau/county.csv', row.names = FALSE)
# write.csv(county_demo_2018, file = 'combined-datasets/county_demo.csv', row.names = F)
```

## TSA

```{r}
# write.csv(merged_tsa, file = 'combined-datasets/tsa.csv', row.names = F)

hosp_tsa = merged_tsa %>% 
  dplyr::select(Date, TSA, TSA_Name, TSA_Combined, Population_DSHS, Ventilators_Available,
                Hospitalizations_Total, Hospitalizations_General, Hospitalizations_ICU,
                Beds_Available_Total, Beds_Available_ICU, Beds_Occupied_Total, Beds_Occupied_ICU) %>% 
  filter(Date >= min(hosp_cap1$Date))
write.csv(hosp_tsa, file = 'tableau/hospitalizations_tsa.csv', row.names = F)
```

## PHR

```{r}
# write.csv(phr_df, file = 'combined-datasets/phr.csv', row.names = F)
```

## HHSC

```{r}
# write.csv(hhsc_df, file = 'original-sources/hhsc.csv', row.names = F)
```

## Metro

```{r}
# write.csv(DSHS_metro, file = 'combined-datasets/metro.csv', row.names = F)
```

## State

```{r}
# state_out = list("longitudinal" = merged_state, "current" = DSHS_state_day)
# write_xlsx(state_out, path = "combined-datasets/state.xlsx")
```

## Demographics

```{r}
write.csv(demo_stacked_combined, file = 'tableau/stacked_demographics.csv', row.names = FALSE)
```

# Performance review

```{r, fig.width=12, fig.height=6}
time_flat = unlist(all_times)
print(time_flat)
names(time_flat) = NULL

time_df = data.frame('time_sec' = time_flat) %>%
  mutate(chunk = as.numeric(rownames(.)) + 2) %>%
  mutate(version = refactor_version) %>%
  mutate(script = 'covid-scraping.rmd') %>%
  arrange(desc(time_sec))

ggplot(time_df, aes(y = time_sec, x = chunk, fill = time_sec)) + 
  geom_bar(stat = 'identity') + 
  geom_text(aes(label = chunk), position=position_dodge(width = 0.9), vjust = -0.25) + 
  labs(x = 'chunk #', y = 'runtime (seconds)') + 
  scale_fill_gradient(low = 'gray80', high = 'tomato1') + 
  theme_pubr() + 
  theme(axis.text.x = element_blank(),
        legend.position = 'none')
```

## total run time

```{r}
time_df %>% summarize(sum(time_sec))
```


```{r}
write.csv(time_df, paste0('diagnostics/scrape_runtime_', refactor_version, '.csv'),
          row.names = FALSE)
```
